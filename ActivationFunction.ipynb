{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配消失問題\n",
    "多層パーセプトロンでは、層の長さを長くすればするほど表現力は増します。一方で、学習が難しくなるという問題が知られています。\n",
    "\n",
    "中間層が10層という深い(deepな)多層パーセプトロンを用いて問題を確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルはヘルパー関数です。\n",
    "\n",
    "# ライブラリのインポート\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "# i 番目のレイヤーの出力を取得する\n",
    "def getActivation(model, i, x):\n",
    "    activations = K.function([model.input, K.learning_phase()], [model.layers[i].output, ])\n",
    "    return activations(x)[0]\n",
    "\n",
    "\n",
    "# iLayer 番目のレイヤーの勾配(誤差)を取得する\n",
    "def getGradientNode(model, i, x, t):\n",
    "    layer_output = model.layers[i].output\n",
    "    loss = K.mean(K.binary_crossentropy(model.output, K.variable(t)))\n",
    "    grads = K.gradients(loss, layer_output)[0]\n",
    "    gradient_function = K.function([model.input], [grads])\n",
    "    return gradient_function([x])[0]\n",
    "\n",
    "\n",
    "# iLayer 番目のレイヤーにつながる重み(wij)の勾配を取得する\n",
    "def getGradientParameter(model, i, x, t):\n",
    "    weights = model.layers[i].weights[0]  # get only kernel (not bias)\n",
    "    gradients = K.gradients(model.total_loss, weights)\n",
    "    input_tensors = model._feed_inputs + model._feed_targets + model._feed_sample_weights\n",
    "    gradient_function = K.function(input_tensors, gradients)\n",
    "    return gradient_function([x, np.ones(len(x)), t, 0])[0]\n",
    "\n",
    "\n",
    "def getRelGradientParameter(model, i, x, t):\n",
    "    return getGradientParameter(model, i, x, t) / model.get_weights()[i * 2]\n",
    "\n",
    "\n",
    "# モデルの勾配や活性化関数の出力等をプロット\n",
    "# 左上: ウェイト(wij)の初期値\n",
    "# 中上: ウェイト(wij)の微分(dE/dwij)\n",
    "# 右上: ウェイト(wij)の微分(dE/dwij)の分散\n",
    "# 左下: 各ノードの出力(sigma(ai))\n",
    "# 中下: 各ノードでの微分(delta_ij)\n",
    "# 右下: 各ノードでの微分(delta_ij)の分散\n",
    "def showGradient(model, x, t):\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(18, 8))\n",
    "\n",
    "    iLayers = [0, 3, 6, 10]\n",
    "    labels = [\n",
    "        ' 0th layer',\n",
    "        ' 3th layer',\n",
    "        ' 6th layer',\n",
    "        'Last layer',\n",
    "    ]\n",
    "\n",
    "    values = [model.get_weights()[i * 2].flatten() for i in iLayers]\n",
    "    ax[0][0].hist(values, bins=50, stacked=False, density=True, label=labels, histtype='step')\n",
    "    ax[0][0].set_xlabel('weight')\n",
    "    ax[0][0].set_ylabel('Probability density')\n",
    "    ax[0][0].legend(loc='upper left', fontsize='x-small')\n",
    "\n",
    "    values = [getActivation(model, i, x).flatten() for i in iLayers]\n",
    "    ax[1][0].hist(values, bins=50, stacked=False, density=True, label=labels, histtype='step')\n",
    "    ax[1][0].set_xlabel('activation')\n",
    "    ax[1][0].set_ylabel('Probability density')\n",
    "    ax[1][0].legend(loc='upper center', fontsize='x-small')\n",
    "\n",
    "    grads = [np.abs(getGradientParameter(model, i, x, t).flatten()) for i in iLayers]\n",
    "    grads = [np.log10(x[x > 0]) for x in grads]\n",
    "    ax[0][1].hist(grads, bins=50, stacked=False, density=True, label=labels, histtype='step')\n",
    "    ax[0][1].set_xlabel('log10(|gradient of weights|)')\n",
    "    ax[0][1].set_ylabel('Probability density')\n",
    "    ax[0][1].legend(loc='upper left', fontsize='x-small')\n",
    "\n",
    "    grads = [np.abs(getGradientNode(model, i, x, t).flatten()) for i in iLayers]\n",
    "    grads = [np.log10(x[x > 0]) for x in grads]\n",
    "    ax[1][1].hist(grads, bins=50, stacked=False, density=True, label=labels, histtype='step')\n",
    "    ax[1][1].set_xlabel('log10(|gradient of nodes|)')\n",
    "    ax[1][1].set_ylabel('Probability density')\n",
    "    ax[1][1].legend(loc='upper left', fontsize='x-small')\n",
    "\n",
    "    var = [np.abs(getGradientParameter(model, i, x, t).flatten()).var() for i in range(10)]\n",
    "    ax[0][2].plot(var)\n",
    "    ax[0][2].set_yscale('log')\n",
    "    ax[0][2].set_xlabel('Layer')\n",
    "    ax[0][2].set_ylabel('Variance of gradient of weights')\n",
    "\n",
    "    var = [np.abs(getGradientNode(model, i, x, t).flatten()).var() for i in range(10)]\n",
    "    ax[1][2].plot(var)\n",
    "    ax[1][2].set_yscale('log')\n",
    "    ax[1][2].set_xlabel('Layer')\n",
    "    ax[1][2].set_ylabel('Variance of gradient of nodes')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.initializers import RandomNormal, RandomUniform\n",
    "from keras.layers import Dense\n",
    "\n",
    "# データセットの生成\n",
    "nSamples = 1000\n",
    "nFeatures = 50\n",
    "x = np.random.randn(nSamples, nFeatures)  # 100個の入力変数を持つイベント1000個生成。それぞれの入力変数は正規分布に従う\n",
    "t = np.random.randint(2, size=nSamples).reshape([nSamples, 1])  # 正解ラベルは0 or 1でランダムに生成\n",
    "\n",
    "# モデルの定義\n",
    "activation = 'sigmoid'  # 中間層の各ノードで使う活性化関数\n",
    "initializer = RandomNormal(mean=0.0, stddev=1.0)  # weight(wij)の初期値。ここでは正規分布に従って初期化する\n",
    "# initializer = RandomUniform(minval=-1, maxval=1)  # weight(wij)の初期値。ここでは一様分布に従って初期化する\n",
    "\n",
    "# 中間層が10層の多層パーセプトロン。各レイヤーのノード数は全て50。\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation=activation, kernel_initializer=initializer, input_dim=nFeatures))\n",
    "for i in range(9):\n",
    "    model.add(Dense(50, activation=activation, kernel_initializer=initializer))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer=initializer))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# モデルの勾配や活性化関数の出力等をプロット\n",
    "# 左上: ウェイト(wij)の初期値\n",
    "# 中上: ウェイト(wij)の微分(dE/dwij)\n",
    "# 右上: ウェイト(wij)の微分(dE/dwij)の分散\n",
    "# 左下: 各ノードの出力(sigma(ai))\n",
    "# 中下: 各ノードでの微分(delta_ij)\n",
    "# 右下: 各ノードでの微分(delta_ij)の分散\n",
    "showGradient(model, x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "左上のプロットはパラメータ($w_{ij}$)の初期値を表しています。指定したとおり、各層で正規分布に従って初期化されています。\n",
    "左下のプロットは活性化関数の出力($z_i$)を表しています。パラメータ($w_{ij}$)の初期値として正規分布を指定すると、シグモイド関数の出力はそのほとんどが0か1に非常に近い値となっています。シグモイド関数の微分は$\\sigma^{'}(x)=\\sigma(x)\\cdot(1-\\sigma(x))$なので、$\\sigma(x)$が0や1に近いときは微分値も非常に小さな値となります。\n",
    "誤差逆伝播の式は\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_{i}^{(k)} &= \\sigma^{'}(a_i^{(k)}) \\left( \\sum_j w_{ij}^{(k+1)} \\cdot \\delta_{j}^{(k+1)} \\right) \\\\\n",
    "\\frac{\\partial E_n}{\\partial w_{ij}^{(k)}}  &= \\delta_{j}^{(k)} \\cdot z_{i}^{(k)}\n",
    "\\end{align}\n",
    "$$\n",
    "でした。$\\sigma^{'}(a_i^{(k)})$が小さいと後方の層から前方の層に誤差が伝わる際に、値が小さくなってしまいます。\n",
    "中上、中下のプロットはそれぞれ各層での$\\frac{\\partial E_n}{\\partial w_{ij}^{(k)}}$、$\\delta_{i}^{(k)}$を表しています。\n",
    "前方の層(0th layer)は後方の層と比較して分布の絶対値が小さくなっています。\n",
    "右上と右下のプロットは各レイヤーでの$\\frac{\\partial E_n}{\\partial w_{ij}^{(k)}}$と$\\delta_{i}^{(k)}$の分布の分散を表しています。\n",
    "\n",
    "このように誤差が前の層にいくにつれて小さくなるため、前の層が後ろの層と比較して学習が進まなくなります。\n",
    "この問題は勾配消失の問題として知られています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配消失はパラメータの初期値や、活性化関数を変更することによって解決・緩和することがわかっています。\n",
    "Kerasの\n",
    "- [初期化のページ](https://keras.io/ja/initializers/)\n",
    "- [活性化関数のページ](https://keras.io/ja/activations/)\n",
    "\n",
    "も参考にしながら、この問題の解決を試みてみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数・パラメータの初期化方法の変更はそれぞれコード中の\"activation\"、\"initializer\"を変更することによって行えます。\n",
    "\n",
    "例えばパラメータの初期化を(-0.01, +0.01)の一様分布に変更するときは以下のコードのようにすれば良いです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.initializers import RandomNormal, RandomUniform\n",
    "from keras.layers import Dense\n",
    "\n",
    "# データセットの生成\n",
    "nSamples = 1000\n",
    "nFeatures = 50\n",
    "x = np.random.randn(nSamples, nFeatures)  # 100個の入力変数を持つイベント1000個生成。それぞれの入力変数は正規分布に従う\n",
    "t = np.random.randint(2, size=nSamples).reshape([nSamples, 1])  # 正解ラベルは0 or 1でランダムに生成\n",
    "\n",
    "# モデルの定義\n",
    "activation = 'sigmoid'  # 中間層の各ノードで使う活性化関数\n",
    "# initializer = RandomNormal(mean=0.0, stddev=1.0)  # weight(wij)の初期値。ここでは正規分布に従って初期化する\n",
    "initializer = RandomUniform(minval=-0.01, maxval=0.01)  # weight(wij)の初期値。ここでは一様分布に従って初期化する\n",
    "\n",
    "# 中間層が10層の多層パーセプトロン。各レイヤーのノード数は全て50。\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation=activation, kernel_initializer=initializer, input_dim=nFeatures))\n",
    "for i in range(9):\n",
    "    model.add(Dense(50, activation=activation, kernel_initializer=initializer))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer=initializer))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# モデルの勾配や活性化関数の出力等をプロット\n",
    "# 左上: ウェイト(wij)の初期値\n",
    "# 中上: ウェイト(wij)の微分(dE/dwij)\n",
    "# 右上: ウェイト(wij)の微分(dE/dwij)の分散\n",
    "# 左下: 各ノードの出力(sigma(ai))\n",
    "# 中下: 各ノードでの微分(delta_ij)\n",
    "# 右下: 各ノードでの微分(delta_ij)の分散\n",
    "showGradient(model, x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この例では活性化関数の出力が0.5付近に集中しています。\n",
    "どのノードも同じ出力をしているということはノード数を増やした意味があまりなくなっており、多層パーセプトロンの表現力が十分に活かしきれていないことがわかります。\n",
    "また、勾配消失も先程の例と比較して大きくなっています。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('myenv': venv)",
   "language": "python",
   "name": "python37364bitmyenvvenv357140d6b8384aec8a2ca97fa481d04d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
